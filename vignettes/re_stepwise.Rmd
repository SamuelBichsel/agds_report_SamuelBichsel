---
title: "Report Exercise Chapter 8"
author: "Samuel Bichsel"
output: html_document
---

## About this file 

This Markdown file is for the report exercise of Chapter 8 (Regression and Classification) of the AGDS Course. The goal of the Exercise is to implement an Algorithm for stepwise forward regression to model GPP in the dataset of half-hourly fluxes. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=4, fig.height=3)



```

```{r} 
library(ggplot2)
library(tidyverse)
library(tidyr)
library(yardstick)
FLUXNET <-  readr::read_csv("./data/df_for_stepwise_regression.csv")


```

## Step 1: Evaluation of all bivariate models (single predictor)

In order to evaluate all single predictor bivariate models, we implement the steps 1-3 of the stepwise forward regression algorithm of the AGDS book. This is done with a loop going through the list of variables and computing a linear regression of the GPP measurements with every other variable being used as a single predictor one by one. 

```{r}
models <- list()
RSQUARED <- c()
#empty lists to put the models, the summaries of the regressions, and the AICs of the models into

predictors_i <- colnames(FLUXNET)[3:17]
predictors_i = predictors_i[predictors_i != "GPP_NT_VUT_REF"]

#loop doing the linear regression of GPP_NT_VUT_REF and then all other predictors, and then putting these models into the models list
for (i in 1:length(predictors_i)){ 
  
  models[[i]] <- 
    lm(FLUXNET$GPP_NT_VUT_REF ~., FLUXNET[ , predictors_i[i]])
  RSQUARED[i] <- summary(models[[i]])$adj.r.squared
}

bestfitting <- models[[which.max(RSQUARED)]]

bestmodel <- c(predictors_i[which.max(RSQUARED)],
               RSQUARED[which.max(RSQUARED)],
               extractAIC(bestfitting)[2])
names(bestmodel) <- (c("variable name", "R Squared", "AIC"))

knitr::kable(bestmodel, caption = "*Table 1: Best single Predictor model and its R-Squared and AIC*")

```

The best model is the one using PPFD_IN (Incoming photosynthetic photon flux density) as a single predictor. The adjusted R-Squared is 0.452, meaning 45.2% of the variability in GPP can be explained by the Regression model with the PPFD variable. The AIC is 29'689.5 which is hard to really interpret without putting it in relation to other models. 

### Plots

As a next step, we create the correlation plots used to visualize the linear regressions from before. This is yet again done with looping through all variables as single predictors and putting all plots into an originally empty list and then priting said list. 

```{r, echo=FALSE,results='hide',fig.keep='all', fig.width = 2.25, fig.height = 1.75}

plots <- list()
#empty list for the plots to be put into


xlabels <- expression("Air Temperature (C)", 
                paste("Shortwave radiation (W m"^-2, ")"),
                paste("Longwave radiation (W m"^-2, ")"), 
                "Vapor Pressure deficit (hPa)", 
                "Atmospheric Pressure (kPa)", "Precipitation(mm)",
                paste("Wind Speed (m s"^-1, ")"), 
                "Air Temperature (Â°C), MDS-Gapfilled", 
                paste("Shortwave radiation (W m"^-2, "), MDS-Gapfilled"),
                paste("Longwave radiation (W m"^-2, "), MDS-Gapfilled"),
                "Vapor Pressure deficit (hPa), MDS-Gapfilled",
                paste("CO"[2], "mole fraction(mol CO"[2], " m"^-2, "s"^-1,                    ")MDS-Gapfilled"), 
                paste("Incoming PPFD (", mu, "mol Photon m"^-2, "s"^-1,")"
                ),
                paste("Friction velocity (m ", "s"^-1, ")"))

#list of the labels to be used 


for (i in 1:length(predictors_i)){

  plots[[i]] <- ggplot(
    data = FLUXNET, 
    aes_string(x = predictors_i[i], y = "GPP_NT_VUT_REF")) +
    geom_point(size = 0.4) +
    geom_smooth(method = "lm", color = "red", size = 0.5) + 
    labs (title = predictors_i[i],
          x = xlabels[i],
          y =  expression(paste("GPP (", mu,"mol CO"[2], " m"^-2, "s"^-1, ")"))) +   
    theme(title = element_text(size = 9), 
          axis.text = element_text(size = 6), 
          axis.title = element_text(size = 7)) 
    
}
  
  




#cowplot::plot_grid(plots, ncol = 4)
plots
#on a sidenote, I couldn't get cowplot to work in order to arrange the plots (it always resulted in a blank square), same story with trying to use theme(title = element_text(size = ...)) to change the title size, I just couldn't get it to work

```


*Figure 1: Correlation plots using single predictors to predict GPP values*


As can be seen from the summaries as well as the graphics, the best R-Squared is the one for the bivariate model using PPFD_IN (Incoming photosynthetic photon flux density) as a single predictor for the GPP (0.452). This means 45.2% of the variability in GPP can be explained by the Regression model with the PPFD_IN variable.


## Step 2: stepwise forward regression  
For the complete stepwise forward regression (aka using the entire algorithm from the book), two loops are used: inner one going through all predictors and choosing the best possible model (going by R-Squared), and an outer one which takes these variables from the inner loop and makes a regression using more and more variables every time, and stops doing that once the model with the best AIC is found. 




```{r, message=FALSE, results='hide'}


used_var <- list()
var_selection <- list()
RSQUARED <- list()
models1 <- list()
AICS <- c()
optimal_model <- c()

#set up list of predictors to be used & predictors being used
predictors_i <- colnames(FLUXNET)[3:17]
predictors_i = predictors_i[predictors_i != "GPP_NT_VUT_REF"]


GPP <- FLUXNET$GPP_NT_VUT_REF

for (i in 1:14){
  
  #inside loop, going through all variables that are still left, computing the R2 of them + the used variables and computing the R2 for all of them
  for (j in 1:length(predictors_i)){
    var_selection[[j]]<- lm(
      as.formula(paste("GPP ~", paste(c(used_var, 
      predictors_i[[j]]), collapse="+"))), 
      data = FLUXNET) 
    RSQUARED[j] <- summary(var_selection[[j]])$adj.r.squared 
  }
  RSQUARED <- as.numeric(RSQUARED)
  x <- which.max(RSQUARED) #which variable gives the biggest R2 
  used_var[[i]] <-  predictors_i[x]
  predictors_i <- predictors_i[-x] #remove the used variables from the list of available variables
  RSQUARED <- RSQUARED[-x]
  
  models1[[i]] <- lm(formula = 
      as.formula(paste("GPP ~", paste(used_var, collapse="+"))), data = FLUXNET) 
  AICS[i] <- as.numeric(extractAIC(models1[[i]])[2])
  
  print((lapply(models1, summary)[i]))
  
  
  if(length(AICS) > 1){
     if(AICS[i] > AICS[i-1])
       
       break}
  
}
```

```{r}

optimal_model <- models1[i-1]
lapply(optimal_model, summary)

```
*Table 2: summary of optimal multiple linear regression model to predict GPP values, using AIC as a indicator*

```{r}
knitr::kable(AICS[i-1], caption = "*Table 3: AICS of optimal multiple linear regression model", align = "l")

#optimal model has to be at spot i-1 because the loop goes until the AICS of the model is worse than the previous one, meaning the last model will be slightly worse than the one right before 

#var_selection is a list of all lm models that we can choose from. used_var is all the variables already used in the previous steps of the algorithms, and with the inner loop j and predictors_i[j] we test that old model + every variable not used yet and keep the best one


```

The optimal model uses 11 of the 14 independent variables, with an adjusted R-Squared of 0.5964 and an AICS of 27397.68. Most variables are statistically significant at the 0.1% level. The 3 variables that were not used are all MDS Gap filled, which makes sense as to why they are not being used since they are very close to the non-gap filled variables and thus barely or not at all explain more variation. 

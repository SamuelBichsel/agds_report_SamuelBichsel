---
title: "Report Exercise Chapter 10"
author: "Samuel Bichsel"
output: html_document
---

## About this file

This Markdown file is for the report exercise of Chapter 10 (Supervised Machine learning II) of the AGDS Course. This final exercise is about spatial upscaling of a prediction model (using training data from one site to predict test data from a different site) and checking how generalisable that model is. Just like the previous chapter, this is done with knn and FLUXNET data. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(dplyr)
library(tidyverse)
library(recipes)
library(caret)
library(cowplot)
```


## Setup 

Setup like said in the book, except use prop = 0.8 to keep 20% of the data for testing purposes. 
```{r}

#make the data wrangling part be a function so I can apply it to both datasets without using way too much code. Yes I am aware this is not very nice looking code and could have been done more optimally but if I can be honest pipe operators confuse me for some reason so I just did it the fastest way I could come up with 

wrangle <- function(data){
data |>
   # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
}



daily_fluxes_Dav <- wrangle(daily_fluxes_Dav <- readr::read_csv(".././data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv"))  

daily_fluxes_Lae <- wrangle(daily_fluxes_Lae <- readr::read_csv(".././data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv"))
  


#so apparently the entire column P_F of daily_fluxes_Lae is of poor quality according to its _QC in the csv document, meaning every value is now NA. This means I have no choice but to drop the entire column :/
daily_fluxes_Lae <- daily_fluxes_Lae[!(colnames(daily_fluxes_Lae)) == "P_F"]

#like previously, I drop the LW_IN_F column as well from the Davos dataset because of the quantity of missing data an a priori not too important nature of the variable 
daily_fluxes_Dav <- daily_fluxes_Dav[!(colnames(daily_fluxes_Dav)) == "LW_IN_F"]



# Data splitting
set.seed(123)  # for reproducibility
#



Dav_split <- rsample::initial_split(daily_fluxes_Dav, prop = 0.8, strata = "VPD_F")
Lae_split <- rsample::initial_split(daily_fluxes_Lae, prop = 0.8, strata = "VPD_F")

daily_fluxes_Dav_train <- rsample::training(Dav_split)
daily_fluxes_Dav_test <- rsample::testing(Dav_split)

daily_fluxes_Lae_train <- rsample::training(Lae_split)
daily_fluxes_Lae_test <- rsample::testing(Lae_split)

# The same model formulation is in the previous chapter
pp_Dav <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_Dav_train) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

pp_Lae <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_Lae_train) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())


```

For tuning and selecting k, we use trainControl(method = "cv") to enable resampling and going through different values of k in order to find the best possible model.

```{r}

mod_Dav <- caret::train(pp_Dav, 
                    data = daily_fluxes_Dav_train |> drop_na(), 
                    method = "knn",
                    trControl = caret::trainControl(method = "cv", 
                                                    number = 60
                                                    ),
                    tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                    metric = "MAE"
                    )

mod_Lae <- caret::train(pp_Lae, 
                    data = daily_fluxes_Lae_train |> drop_na(), 
                    method = "knn",
                    trControl = caret::trainControl(method = "cv", 
                                                    number = 60
                                                    ),
                    tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                    metric = "MAE"
                    )


mod_Dav
mod_Lae

```
Using MAE as a metric to tune the k, the best values are k = 25 for the Models of Davos and Laegern respectively. 



## Evaluate models 

#### Compare within-site predictions and across-site predictions on the test set for both sites, considering different metrics.

Just like for the exercises of the previous chapter, we use the function eval_model to compare the models. Since there are 2 locations with training and testing data for each, this gives 4 different combinations of training and testing (two within-site, and two across-site predictions).

```{r}
source(".././R/eval_model.R")

#adding the fitted values by hand since eval_model doesn't do that for me

daily_fluxes_Dav_test <- daily_fluxes_Dav_test |> 
    drop_na()

daily_fluxes_Dav_test$Dav_fitted <- predict(mod_Dav, newdata = daily_fluxes_Dav_test)

daily_fluxes_Dav_test$Lae_fitted <- predict(mod_Lae, newdata = daily_fluxes_Dav_test)



daily_fluxes_Lae_test <- daily_fluxes_Lae_test |> 
    drop_na()

daily_fluxes_Lae_test$Lae_fitted <- predict(mod_Lae, newdata = daily_fluxes_Lae_test)

daily_fluxes_Lae_test$Dav_fitted <- predict(mod_Dav, newdata = daily_fluxes_Lae_test)


```

#### Davos training, Davos testing set
```{r, fig.cap = "*Figure 1: Evaluation of knn model fit vs. GPP measurements using the Davos training set to predict the Davos test set*"}
eval_model(mod = mod_Dav, df_train = daily_fluxes_Dav_train, df_test = daily_fluxes_Dav_test)

knitr::kable(daily_fluxes_Dav_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, Dav_fitted), caption = "*Table 1: Metrics of knn model fit vs. GPP measurements using the Davos training set to predict the Davos test set*")

```

#### Davos training, Laegern testing set
```{r, fig.cap = "*Figure 2: Evaluation of knn model fit vs. GPP measurements using the Davos training set to predict the Laegern test set*"}
eval_model(mod = mod_Dav, df_train = daily_fluxes_Dav_train, df_test = daily_fluxes_Lae_test)

knitr::kable(daily_fluxes_Lae_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, Dav_fitted), caption = "*Table 2: Metrics of knn model fit vs. GPP measurements using the Davos training set to predict the Laegern test set*")

```


#### Lagern training, Laegern testing set
```{r, fig.cap = "*figure 3: Evaluation of knn model fit vs. GPP measurements using the Laegern training set to predict the Laegern test set*"}
eval_model(mod = mod_Lae, df_train = daily_fluxes_Lae_train, df_test = daily_fluxes_Lae_test)

knitr::kable(daily_fluxes_Lae_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, Lae_fitted), caption = "*Table 3: Metrics of knn model fit vs. GPP measurements using the Laegern training set to predict the Laegern test set*")

```



#### Laegern training, Davos testing set
```{r, fig.cap = "*Figure 4: Evaluation of knn model fit vs. GPP measurements using the Laegern training set to predict the Davos test set*"}
eval_model(mod = mod_Lae, df_train = daily_fluxes_Lae_train, df_test = daily_fluxes_Dav_test)

knitr::kable(daily_fluxes_Dav_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, Lae_fitted), caption = "*Table 4: Metrics of knn model fit vs. GPP measurements using the Davos training set to predict the Davos test set*")


```



The results for these across-site model predictions aren't too bad, but definitely below the within-site, with all 3 metrics being slightly worse. 


## Model with pooled data 

#### Train a single model with training data pooled from both sites and predict with this single model on the test data of both sites

With the pooling of the Laegern and Davos datasets, there also needs to be a new training model (with caret::train) mod_pool. The procedure is the same as for the previous datasets.

```{r, fig.cap = "*Figure 5: Evaluation of GPP knn model fit vs. GPP measurements using pooled data to predict the Davos test data*"}
# pool data
#before being able to pool the data, I need to drop the columns that don't appear in both as otherwise I cannot combine the dataframes
daily_fluxes_Dav_train <- daily_fluxes_Dav_train[!(colnames(daily_fluxes_Dav_train)) == "P_F"]

daily_fluxes_Lae_train <- daily_fluxes_Lae_train[!(colnames(daily_fluxes_Lae_train)) == "LW_IN_F"]



daily_fluxes_train_pool <- rbind(daily_fluxes_Dav_train, daily_fluxes_Lae_train)


pp_pool <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F,
                      data = daily_fluxes_train_pool) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

mod_pool <- caret::train(pp_pool, 
                    data = daily_fluxes_train_pool |> drop_na(), 
                    method = "knn",
                    trControl = caret::trainControl(method = "cv", 
                                                    number = 60
                                                    ),
                    tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                    metric = "MAE"
                    )

mod_pool
eval_model(mod = mod_pool, df_train = daily_fluxes_train_pool, df_test = daily_fluxes_Dav_test)
```


```{r, fig.cap = "*Figure 6: Evaluation of GPP knn model fit vs. GPP measurements using pooled data to predict the Davos*"}
eval_model(mod = mod_pool, df_train = daily_fluxes_train_pool, df_test = daily_fluxes_Lae_test)


``` 


It seems as if with a pooled training dataset, the R-Squared is quite high for training but the testing isn't really any different from doing it with the separated datasets. This comes potentially from the fact that k = 2 is the optimal model according to the MAE, so it is quite high on the bias side in terms of bias-variance tradeoff. I am not quite sure if that's normal though and there might have just been a mistake in implementation leading to this (or maybe this is normal after all).  


## Comparing the the two sites 

#### Get information about the characteristics of the two sites. What are the differences in terms of climate, vegetation, altitude, etc. between the Davos and Laegern sites? Interpret biases of the out-of-sample predictions with a view to the site characteristics.

Davos: 1639 m above sea level
Laegern: 689 m above sea level

Davos is in the alps, Laegern in the Swiss pleateau

Vegetation: 
Davos: Evergreen Needleleaf Forest, meaning most trees remain green even in Winter
Laegern: Mixed Forest without any overly dominant forest type

Annual Temp:
Davos: 2.8°C
Laegern: 8.3°C

Annual Precipitation:
Davos: 1062 mm 
Laegern: 1100 mm

To compare the out.of-sample predictions of the two sites, we once again turn to temporal variations of fits vs. GPP measurements. To both sites measurements we compare the within-site and across-site predictions, and hope to find some consistent differences in the predictions that could be explained by the differences in the site characteristics.

```{r, fig.width = 13,fig.height = 5, fig.cap = "*Figure 5: Time variation for the site of Davos of measured GPP and precictions using both sites training data*"}

colors1 <- c("GPP Measured" = "black", "Davos Prediction" = "#e41d1d", "Laegern Prediction" = "#357cbc")

yearstart <- daily_fluxes_Dav_test[match(unique(year(daily_fluxes_Dav_test$TIMESTAMP)), year(daily_fluxes_Dav_test$TIMESTAMP)),][,1]

yearstart$TIMESTAMP <- as.numeric(yearstart$TIMESTAMP)
#select the first date with an obs of every year and then put a line there on the plots as to make it easier to read when the year starts and thus easier to see the results



ggplot() +
  geom_point(data = daily_fluxes_Dav_test, aes(x = TIMESTAMP, y = GPP_NT_VUT_REF,  color = "GPP Measured"), alpha = 0.8) +
  geom_line(data = daily_fluxes_Dav_test, aes(x = TIMESTAMP, y = Dav_fitted,  color = "Davos Prediction"), alpha = 0.5) +
  geom_line(data = daily_fluxes_Dav_test, aes(x = TIMESTAMP, y = Lae_fitted,  color = "Laegern Prediction"), alpha = 0.5) +

    scale_color_manual(values = colors1) +
  geom_vline(xintercept = yearstart$TIMESTAMP, color = "black", linewidth = 0.3) + 
  
  labs(x = "Year", y = "GPP") +
  
  theme(axis.text = element_text(size = 12), axis.title = element_text(size = 15)) + 
  
  theme(legend.key.size = unit(1, "cm"), 
    legend.text = element_text(size = 10), legend.title = element_blank())

#note: the start of year look really weird in the middle due to there only being very little data for 2005
```


In this first graphic about the measured GPP vs. predictions for the Davos test set we can see a few things:
1) The across-site predictions using the Laegern training data overestimate the GPP by quite a big margin in summer. 
2) In winter/early spring, the predictions using Laegern training data seem to consistently "go back up" from the winterly low a few weeks earlier than the actual data and Davos predictions. 

These diffrences could, in part, be due to the differences in vegetation, as with similar Temperature, Radiation etc. mixed forest with more deciduous trees may have a greater GPP than full evergreen needleleaf forests.   


```{r, fig.width = 9,fig.height = 4, fig.cap = "*Figure 6: Time variation for the site of Davos of measured GPP and precictions using both sites training data*"}

colors1 <- c("GPP Measured" = "black", "Laegern Prediction" = "#357cbc", "Davos Prediction" = "#e41d1d")

yearstart2 <- daily_fluxes_Lae_test[match(unique(year(daily_fluxes_Lae_test$TIMESTAMP)), year(daily_fluxes_Lae_test$TIMESTAMP)),][,1]


yearstart2$TIMESTAMP <- as.numeric(yearstart2$TIMESTAMP)



ggplot() +
  geom_point(data = daily_fluxes_Lae_test, aes(x = TIMESTAMP, y = GPP_NT_VUT_REF,  color = "GPP Measured"), alpha = 0.8) +
  geom_line(data = daily_fluxes_Lae_test, aes(x = TIMESTAMP, y = Lae_fitted,  color = "Laegern Prediction"), alpha = 0.5) +
  geom_line(data = daily_fluxes_Lae_test, aes(x = TIMESTAMP, y = Dav_fitted,  color = "Davos Prediction"), alpha = 0.5) +

    scale_color_manual(values = colors1) +
  geom_vline(xintercept = yearstart2$TIMESTAMP, color = "black", linewidth = 0.3) +
  
  labs(x = "Year", y = "GPP") +
  
  theme(axis.text = element_text(size = 8), axis.title = element_text(size = 10)) + 
  
  theme(legend.key.size = unit(1, "cm"), 
    legend.text = element_text(size = 8), legend.title = element_blank())


```

When looking at the predictions onto the Laegern dataset, we can come to the same conclusions: the Laegern predictions (as well as the measurements) go way above the predictions using Davos training data in summer, indicating that the gross primary production is higher in Laegern than in Davos in summer even when every external factor is kept even. Without further research, it is hard to make any clear call whether or not this is due to the forest type or if other factors play more of a role in shaping these differences. 
